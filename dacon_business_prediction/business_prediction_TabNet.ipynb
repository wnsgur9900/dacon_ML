{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data-files/train.csv\")\n",
    "test = pd.read_csv(\"data-files/test.csv\")\n",
    "submission = pd.read_csv(\"data-files/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "except_cols = [\"ID\", \"성공확률\"]\n",
    "X_train = train.drop(columns=except_cols)\n",
    "y_train = train[\"성공확률\"]\n",
    "\n",
    "cat_cols = X_train.select_dtypes(include=\"object\").columns.tolist()\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols)\n",
    "X_test = test.drop(columns=\"ID\")\n",
    "X_test = pd.get_dummies(X_test, columns=cat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.align(X_test, join=\"left\", axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split 하기 전에\n",
    "X_train = X_train.fillna(X_train.median())\n",
    "X_test = X_test.fillna(X_test.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "설립연도                   0\n",
       "직원 수                   0\n",
       "고객수(백만명)               0\n",
       "총 투자금(억원)              0\n",
       "연매출(억원)                0\n",
       "SNS 팔로워 수(백만명)         0\n",
       "국가_CT001               0\n",
       "국가_CT002               0\n",
       "국가_CT003               0\n",
       "국가_CT004               0\n",
       "국가_CT005               0\n",
       "국가_CT006               0\n",
       "국가_CT007               0\n",
       "국가_CT008               0\n",
       "국가_CT009               0\n",
       "국가_CT010               0\n",
       "분야_AI                  0\n",
       "분야_게임                  0\n",
       "분야_기술                  0\n",
       "분야_물류                  0\n",
       "분야_에너지                 0\n",
       "분야_에듀테크                0\n",
       "분야_이커머스                0\n",
       "분야_푸드테크                0\n",
       "분야_핀테크                 0\n",
       "분야_헬스케어                0\n",
       "투자단계_IPO               0\n",
       "투자단계_Seed              0\n",
       "투자단계_Series A          0\n",
       "투자단계_Series B          0\n",
       "투자단계_Series C          0\n",
       "인수여부_No                0\n",
       "인수여부_Yes               0\n",
       "상장여부_No                0\n",
       "상장여부_Yes               0\n",
       "기업가치(백억원)_1500-2500    0\n",
       "기업가치(백억원)_2500-3500    0\n",
       "기업가치(백억원)_3500-4500    0\n",
       "기업가치(백억원)_4500-6000    0\n",
       "기업가치(백억원)_6000이상       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\python-env-311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "model = TabNetRegressor(\n",
    "    cat_idxs=[],        # get_dummies 했으니까 빈 리스트\n",
    "    cat_dims=[],\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\python-env-311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.54889 |  0:00:00s\n",
      "epoch 1  | loss: 0.1845  |  0:00:00s\n",
      "epoch 2  | loss: 0.12609 |  0:00:01s\n",
      "epoch 3  | loss: 0.10155 |  0:00:01s\n",
      "epoch 4  | loss: 0.08987 |  0:00:01s\n",
      "epoch 5  | loss: 0.07766 |  0:00:01s\n",
      "epoch 6  | loss: 0.07138 |  0:00:02s\n",
      "epoch 7  | loss: 0.06909 |  0:00:02s\n",
      "epoch 8  | loss: 0.06863 |  0:00:02s\n",
      "epoch 9  | loss: 0.06771 |  0:00:02s\n",
      "epoch 10 | loss: 0.06393 |  0:00:03s\n",
      "epoch 11 | loss: 0.06273 |  0:00:03s\n",
      "epoch 12 | loss: 0.06269 |  0:00:03s\n",
      "epoch 13 | loss: 0.06226 |  0:00:04s\n",
      "epoch 14 | loss: 0.0615  |  0:00:04s\n",
      "epoch 15 | loss: 0.06049 |  0:00:04s\n",
      "epoch 16 | loss: 0.06099 |  0:00:04s\n",
      "epoch 17 | loss: 0.06033 |  0:00:05s\n",
      "epoch 18 | loss: 0.06041 |  0:00:05s\n",
      "epoch 19 | loss: 0.0603  |  0:00:05s\n",
      "epoch 20 | loss: 0.06081 |  0:00:06s\n",
      "epoch 21 | loss: 0.06062 |  0:00:06s\n",
      "epoch 22 | loss: 0.06003 |  0:00:06s\n",
      "epoch 23 | loss: 0.05959 |  0:00:06s\n",
      "epoch 24 | loss: 0.05976 |  0:00:07s\n",
      "epoch 25 | loss: 0.05934 |  0:00:07s\n",
      "epoch 26 | loss: 0.05985 |  0:00:07s\n",
      "epoch 27 | loss: 0.05917 |  0:00:08s\n",
      "epoch 28 | loss: 0.05936 |  0:00:08s\n",
      "epoch 29 | loss: 0.05919 |  0:00:08s\n",
      "epoch 30 | loss: 0.0585  |  0:00:08s\n",
      "epoch 31 | loss: 0.05881 |  0:00:09s\n",
      "epoch 32 | loss: 0.05855 |  0:00:09s\n",
      "epoch 33 | loss: 0.05853 |  0:00:09s\n",
      "epoch 34 | loss: 0.05824 |  0:00:10s\n",
      "epoch 35 | loss: 0.05838 |  0:00:10s\n",
      "epoch 36 | loss: 0.05821 |  0:00:10s\n",
      "epoch 37 | loss: 0.05854 |  0:00:10s\n",
      "epoch 38 | loss: 0.05817 |  0:00:11s\n",
      "epoch 39 | loss: 0.05791 |  0:00:11s\n",
      "epoch 40 | loss: 0.05835 |  0:00:11s\n",
      "epoch 41 | loss: 0.05811 |  0:00:12s\n",
      "epoch 42 | loss: 0.05812 |  0:00:12s\n",
      "epoch 43 | loss: 0.0578  |  0:00:12s\n",
      "epoch 44 | loss: 0.05808 |  0:00:12s\n",
      "epoch 45 | loss: 0.0583  |  0:00:13s\n",
      "epoch 46 | loss: 0.05865 |  0:00:13s\n",
      "epoch 47 | loss: 0.0578  |  0:00:13s\n",
      "epoch 48 | loss: 0.05784 |  0:00:14s\n",
      "epoch 49 | loss: 0.05837 |  0:00:14s\n",
      "epoch 50 | loss: 0.05743 |  0:00:14s\n",
      "epoch 51 | loss: 0.05714 |  0:00:14s\n",
      "epoch 52 | loss: 0.058   |  0:00:15s\n",
      "epoch 53 | loss: 0.05743 |  0:00:15s\n",
      "epoch 54 | loss: 0.05841 |  0:00:15s\n",
      "epoch 55 | loss: 0.05763 |  0:00:15s\n",
      "epoch 56 | loss: 0.05831 |  0:00:16s\n",
      "epoch 57 | loss: 0.05802 |  0:00:16s\n",
      "epoch 58 | loss: 0.0575  |  0:00:16s\n",
      "epoch 59 | loss: 0.05724 |  0:00:16s\n",
      "epoch 60 | loss: 0.05737 |  0:00:17s\n",
      "epoch 61 | loss: 0.05687 |  0:00:17s\n",
      "epoch 62 | loss: 0.05747 |  0:00:17s\n",
      "epoch 63 | loss: 0.05707 |  0:00:17s\n",
      "epoch 64 | loss: 0.05712 |  0:00:18s\n",
      "epoch 65 | loss: 0.05673 |  0:00:18s\n",
      "epoch 66 | loss: 0.05767 |  0:00:18s\n",
      "epoch 67 | loss: 0.05819 |  0:00:19s\n",
      "epoch 68 | loss: 0.05691 |  0:00:19s\n",
      "epoch 69 | loss: 0.05669 |  0:00:19s\n",
      "epoch 70 | loss: 0.05668 |  0:00:19s\n",
      "epoch 71 | loss: 0.05692 |  0:00:20s\n",
      "epoch 72 | loss: 0.05705 |  0:00:20s\n",
      "epoch 73 | loss: 0.05667 |  0:00:20s\n",
      "epoch 74 | loss: 0.05643 |  0:00:20s\n",
      "epoch 75 | loss: 0.05665 |  0:00:21s\n",
      "epoch 76 | loss: 0.05723 |  0:00:21s\n",
      "epoch 77 | loss: 0.05651 |  0:00:21s\n",
      "epoch 78 | loss: 0.0567  |  0:00:21s\n",
      "epoch 79 | loss: 0.05649 |  0:00:22s\n",
      "epoch 80 | loss: 0.05566 |  0:00:22s\n",
      "epoch 81 | loss: 0.05667 |  0:00:22s\n",
      "epoch 82 | loss: 0.05598 |  0:00:23s\n",
      "epoch 83 | loss: 0.0559  |  0:00:23s\n",
      "epoch 84 | loss: 0.05611 |  0:00:23s\n",
      "epoch 85 | loss: 0.05661 |  0:00:23s\n",
      "epoch 86 | loss: 0.05627 |  0:00:24s\n",
      "epoch 87 | loss: 0.05618 |  0:00:24s\n",
      "epoch 88 | loss: 0.05568 |  0:00:24s\n",
      "epoch 89 | loss: 0.05599 |  0:00:24s\n",
      "epoch 90 | loss: 0.05597 |  0:00:25s\n",
      "epoch 91 | loss: 0.05584 |  0:00:25s\n",
      "epoch 92 | loss: 0.05569 |  0:00:25s\n",
      "epoch 93 | loss: 0.05683 |  0:00:25s\n",
      "epoch 94 | loss: 0.05606 |  0:00:26s\n",
      "epoch 95 | loss: 0.05632 |  0:00:26s\n",
      "epoch 96 | loss: 0.05683 |  0:00:26s\n",
      "epoch 97 | loss: 0.05737 |  0:00:27s\n",
      "epoch 98 | loss: 0.05839 |  0:00:27s\n",
      "epoch 99 | loss: 0.05793 |  0:00:27s\n",
      "epoch 100| loss: 0.0573  |  0:00:27s\n",
      "epoch 101| loss: 0.05781 |  0:00:28s\n",
      "epoch 102| loss: 0.05683 |  0:00:28s\n",
      "epoch 103| loss: 0.05755 |  0:00:28s\n",
      "epoch 104| loss: 0.05779 |  0:00:28s\n",
      "epoch 105| loss: 0.05684 |  0:00:29s\n",
      "epoch 106| loss: 0.05701 |  0:00:29s\n",
      "epoch 107| loss: 0.057   |  0:00:29s\n",
      "epoch 108| loss: 0.05717 |  0:00:29s\n",
      "epoch 109| loss: 0.05692 |  0:00:30s\n",
      "epoch 110| loss: 0.05719 |  0:00:30s\n",
      "epoch 111| loss: 0.0567  |  0:00:30s\n",
      "epoch 112| loss: 0.05661 |  0:00:30s\n",
      "epoch 113| loss: 0.05669 |  0:00:31s\n",
      "epoch 114| loss: 0.05652 |  0:00:31s\n",
      "epoch 115| loss: 0.05656 |  0:00:31s\n",
      "epoch 116| loss: 0.05549 |  0:00:32s\n",
      "epoch 117| loss: 0.05654 |  0:00:32s\n",
      "epoch 118| loss: 0.05613 |  0:00:32s\n",
      "epoch 119| loss: 0.0564  |  0:00:32s\n",
      "epoch 120| loss: 0.05516 |  0:00:33s\n",
      "epoch 121| loss: 0.05594 |  0:00:33s\n",
      "epoch 122| loss: 0.05603 |  0:00:33s\n",
      "epoch 123| loss: 0.05534 |  0:00:33s\n",
      "epoch 124| loss: 0.05585 |  0:00:34s\n",
      "epoch 125| loss: 0.05545 |  0:00:34s\n",
      "epoch 126| loss: 0.05625 |  0:00:34s\n",
      "epoch 127| loss: 0.05514 |  0:00:35s\n",
      "epoch 128| loss: 0.05549 |  0:00:35s\n",
      "epoch 129| loss: 0.05502 |  0:00:35s\n",
      "epoch 130| loss: 0.05509 |  0:00:35s\n",
      "epoch 131| loss: 0.05438 |  0:00:36s\n",
      "epoch 132| loss: 0.05537 |  0:00:36s\n",
      "epoch 133| loss: 0.05496 |  0:00:36s\n",
      "epoch 134| loss: 0.05504 |  0:00:36s\n",
      "epoch 135| loss: 0.05596 |  0:00:37s\n",
      "epoch 136| loss: 0.05552 |  0:00:37s\n",
      "epoch 137| loss: 0.0553  |  0:00:37s\n",
      "epoch 138| loss: 0.05495 |  0:00:37s\n",
      "epoch 139| loss: 0.0541  |  0:00:38s\n",
      "epoch 140| loss: 0.05476 |  0:00:38s\n",
      "epoch 141| loss: 0.05371 |  0:00:38s\n",
      "epoch 142| loss: 0.05374 |  0:00:38s\n",
      "epoch 143| loss: 0.05494 |  0:00:39s\n",
      "epoch 144| loss: 0.05439 |  0:00:39s\n",
      "epoch 145| loss: 0.05495 |  0:00:39s\n",
      "epoch 146| loss: 0.05423 |  0:00:40s\n",
      "epoch 147| loss: 0.05346 |  0:00:40s\n",
      "epoch 148| loss: 0.05385 |  0:00:40s\n",
      "epoch 149| loss: 0.05372 |  0:00:40s\n",
      "epoch 150| loss: 0.05381 |  0:00:41s\n",
      "epoch 151| loss: 0.05393 |  0:00:41s\n",
      "epoch 152| loss: 0.05346 |  0:00:41s\n",
      "epoch 153| loss: 0.05268 |  0:00:41s\n",
      "epoch 154| loss: 0.05286 |  0:00:42s\n",
      "epoch 155| loss: 0.05346 |  0:00:42s\n",
      "epoch 156| loss: 0.05381 |  0:00:42s\n",
      "epoch 157| loss: 0.0532  |  0:00:43s\n",
      "epoch 158| loss: 0.05327 |  0:00:43s\n",
      "epoch 159| loss: 0.05373 |  0:00:43s\n",
      "epoch 160| loss: 0.05378 |  0:00:44s\n",
      "epoch 161| loss: 0.05338 |  0:00:44s\n",
      "epoch 162| loss: 0.0531  |  0:00:44s\n",
      "epoch 163| loss: 0.05345 |  0:00:44s\n",
      "epoch 164| loss: 0.05338 |  0:00:45s\n",
      "epoch 165| loss: 0.05361 |  0:00:45s\n",
      "epoch 166| loss: 0.05245 |  0:00:45s\n",
      "epoch 167| loss: 0.05221 |  0:00:46s\n",
      "epoch 168| loss: 0.05297 |  0:00:46s\n",
      "epoch 169| loss: 0.05254 |  0:00:46s\n",
      "epoch 170| loss: 0.05273 |  0:00:46s\n",
      "epoch 171| loss: 0.05304 |  0:00:47s\n",
      "epoch 172| loss: 0.05235 |  0:00:47s\n",
      "epoch 173| loss: 0.05275 |  0:00:47s\n",
      "epoch 174| loss: 0.0519  |  0:00:47s\n",
      "epoch 175| loss: 0.05245 |  0:00:48s\n",
      "epoch 176| loss: 0.05207 |  0:00:48s\n",
      "epoch 177| loss: 0.05126 |  0:00:48s\n",
      "epoch 178| loss: 0.05139 |  0:00:49s\n",
      "epoch 179| loss: 0.0519  |  0:00:49s\n",
      "epoch 180| loss: 0.05146 |  0:00:49s\n",
      "epoch 181| loss: 0.05085 |  0:00:49s\n",
      "epoch 182| loss: 0.05173 |  0:00:50s\n",
      "epoch 183| loss: 0.05202 |  0:00:50s\n",
      "epoch 184| loss: 0.05069 |  0:00:50s\n",
      "epoch 185| loss: 0.052   |  0:00:51s\n",
      "epoch 186| loss: 0.05157 |  0:00:51s\n",
      "epoch 187| loss: 0.05207 |  0:00:51s\n",
      "epoch 188| loss: 0.05157 |  0:00:51s\n",
      "epoch 189| loss: 0.05144 |  0:00:52s\n",
      "epoch 190| loss: 0.05139 |  0:00:52s\n",
      "epoch 191| loss: 0.05127 |  0:00:52s\n",
      "epoch 192| loss: 0.05143 |  0:00:52s\n",
      "epoch 193| loss: 0.05083 |  0:00:53s\n",
      "epoch 194| loss: 0.05215 |  0:00:53s\n",
      "epoch 195| loss: 0.05092 |  0:00:53s\n",
      "epoch 196| loss: 0.05063 |  0:00:53s\n",
      "epoch 197| loss: 0.05196 |  0:00:54s\n",
      "epoch 198| loss: 0.05094 |  0:00:54s\n",
      "epoch 199| loss: 0.05051 |  0:00:54s\n",
      "epoch 200| loss: 0.05033 |  0:00:54s\n",
      "epoch 201| loss: 0.05042 |  0:00:55s\n",
      "epoch 202| loss: 0.04994 |  0:00:55s\n",
      "epoch 203| loss: 0.04977 |  0:00:55s\n",
      "epoch 204| loss: 0.05002 |  0:00:56s\n",
      "epoch 205| loss: 0.04976 |  0:00:56s\n",
      "epoch 206| loss: 0.05024 |  0:00:56s\n",
      "epoch 207| loss: 0.04973 |  0:00:56s\n",
      "epoch 208| loss: 0.0499  |  0:00:57s\n",
      "epoch 209| loss: 0.0501  |  0:00:57s\n",
      "epoch 210| loss: 0.04945 |  0:00:57s\n",
      "epoch 211| loss: 0.04943 |  0:00:57s\n",
      "epoch 212| loss: 0.04984 |  0:00:58s\n",
      "epoch 213| loss: 0.04949 |  0:00:58s\n",
      "epoch 214| loss: 0.04932 |  0:00:58s\n",
      "epoch 215| loss: 0.04942 |  0:00:59s\n",
      "epoch 216| loss: 0.04844 |  0:00:59s\n",
      "epoch 217| loss: 0.04925 |  0:00:59s\n",
      "epoch 218| loss: 0.04995 |  0:00:59s\n",
      "epoch 219| loss: 0.04932 |  0:01:00s\n",
      "epoch 220| loss: 0.0488  |  0:01:00s\n",
      "epoch 221| loss: 0.0496  |  0:01:00s\n",
      "epoch 222| loss: 0.04863 |  0:01:01s\n",
      "epoch 223| loss: 0.04872 |  0:01:01s\n",
      "epoch 224| loss: 0.04765 |  0:01:01s\n",
      "epoch 225| loss: 0.04903 |  0:01:01s\n",
      "epoch 226| loss: 0.04964 |  0:01:02s\n",
      "epoch 227| loss: 0.04977 |  0:01:02s\n",
      "epoch 228| loss: 0.04868 |  0:01:02s\n",
      "epoch 229| loss: 0.04921 |  0:01:02s\n",
      "epoch 230| loss: 0.04795 |  0:01:03s\n",
      "epoch 231| loss: 0.04745 |  0:01:03s\n",
      "epoch 232| loss: 0.04786 |  0:01:03s\n",
      "epoch 233| loss: 0.04816 |  0:01:03s\n",
      "epoch 234| loss: 0.04892 |  0:01:04s\n",
      "epoch 235| loss: 0.04857 |  0:01:04s\n",
      "epoch 236| loss: 0.04888 |  0:01:04s\n",
      "epoch 237| loss: 0.04903 |  0:01:04s\n",
      "epoch 238| loss: 0.04842 |  0:01:05s\n",
      "epoch 239| loss: 0.04855 |  0:01:05s\n",
      "epoch 240| loss: 0.04839 |  0:01:05s\n",
      "epoch 241| loss: 0.04844 |  0:01:06s\n",
      "epoch 242| loss: 0.04821 |  0:01:06s\n",
      "epoch 243| loss: 0.04878 |  0:01:06s\n",
      "epoch 244| loss: 0.0496  |  0:01:06s\n",
      "epoch 245| loss: 0.04879 |  0:01:07s\n",
      "epoch 246| loss: 0.04779 |  0:01:07s\n",
      "epoch 247| loss: 0.04804 |  0:01:07s\n",
      "epoch 248| loss: 0.04854 |  0:01:07s\n",
      "epoch 249| loss: 0.04855 |  0:01:08s\n",
      "epoch 250| loss: 0.04864 |  0:01:08s\n",
      "epoch 251| loss: 0.04927 |  0:01:08s\n",
      "epoch 252| loss: 0.04748 |  0:01:08s\n",
      "epoch 253| loss: 0.04837 |  0:01:09s\n",
      "epoch 254| loss: 0.04805 |  0:01:09s\n",
      "epoch 255| loss: 0.04766 |  0:01:09s\n",
      "epoch 256| loss: 0.04749 |  0:01:10s\n",
      "epoch 257| loss: 0.04816 |  0:01:10s\n",
      "epoch 258| loss: 0.04741 |  0:01:10s\n",
      "epoch 259| loss: 0.04718 |  0:01:10s\n",
      "epoch 260| loss: 0.04692 |  0:01:11s\n",
      "epoch 261| loss: 0.0464  |  0:01:11s\n",
      "epoch 262| loss: 0.0478  |  0:01:11s\n",
      "epoch 263| loss: 0.04725 |  0:01:11s\n",
      "epoch 264| loss: 0.0467  |  0:01:12s\n",
      "epoch 265| loss: 0.04677 |  0:01:12s\n",
      "epoch 266| loss: 0.04666 |  0:01:12s\n",
      "epoch 267| loss: 0.04726 |  0:01:13s\n",
      "epoch 268| loss: 0.04676 |  0:01:13s\n",
      "epoch 269| loss: 0.04754 |  0:01:13s\n",
      "epoch 270| loss: 0.04698 |  0:01:13s\n",
      "epoch 271| loss: 0.04736 |  0:01:14s\n",
      "epoch 272| loss: 0.04641 |  0:01:14s\n",
      "epoch 273| loss: 0.04636 |  0:01:14s\n",
      "epoch 274| loss: 0.04655 |  0:01:14s\n",
      "epoch 275| loss: 0.04615 |  0:01:15s\n",
      "epoch 276| loss: 0.04639 |  0:01:15s\n",
      "epoch 277| loss: 0.04613 |  0:01:15s\n",
      "epoch 278| loss: 0.04574 |  0:01:15s\n",
      "epoch 279| loss: 0.04566 |  0:01:16s\n",
      "epoch 280| loss: 0.04704 |  0:01:16s\n",
      "epoch 281| loss: 0.04654 |  0:01:16s\n",
      "epoch 282| loss: 0.04635 |  0:01:16s\n",
      "epoch 283| loss: 0.04517 |  0:01:17s\n",
      "epoch 284| loss: 0.04618 |  0:01:17s\n",
      "epoch 285| loss: 0.04645 |  0:01:17s\n",
      "epoch 286| loss: 0.04749 |  0:01:18s\n",
      "epoch 287| loss: 0.04598 |  0:01:18s\n",
      "epoch 288| loss: 0.04722 |  0:01:18s\n",
      "epoch 289| loss: 0.04683 |  0:01:18s\n",
      "epoch 290| loss: 0.04646 |  0:01:19s\n",
      "epoch 291| loss: 0.04661 |  0:01:19s\n",
      "epoch 292| loss: 0.04787 |  0:01:19s\n",
      "epoch 293| loss: 0.04675 |  0:01:19s\n",
      "epoch 294| loss: 0.04564 |  0:01:20s\n",
      "epoch 295| loss: 0.04603 |  0:01:20s\n",
      "epoch 296| loss: 0.0457  |  0:01:20s\n",
      "epoch 297| loss: 0.04567 |  0:01:20s\n",
      "epoch 298| loss: 0.04512 |  0:01:21s\n",
      "epoch 299| loss: 0.04486 |  0:01:21s\n"
     ]
    }
   ],
   "source": [
    "# train 전체로 학습\n",
    "model.fit(\n",
    "    X_train=X_train.values,\n",
    "    y_train=y_train.values.reshape(-1, 1),\n",
    "    max_epochs=300,\n",
    "    patience=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 데이터 예측\n",
    "y_pred_test = model.predict(X_test.values)\n",
    "y_pred_test = np.round(y_pred_test.flatten(), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['성공확률'] = y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_TabNet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# MAE 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\python-env-311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    }
   ],
   "source": [
    "model2 = TabNetRegressor(\n",
    "    cat_idxs=[],\n",
    "    cat_dims=[],\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\human\\.conda\\envs\\python-env-311\\Lib\\site-packages\\pytorch_tabnet\\abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70856 |  0:00:00s\n",
      "epoch 1  | loss: 0.25161 |  0:00:00s\n",
      "epoch 2  | loss: 0.16742 |  0:00:00s\n",
      "epoch 3  | loss: 0.13023 |  0:00:01s\n",
      "epoch 4  | loss: 0.10438 |  0:00:01s\n",
      "epoch 5  | loss: 0.0886  |  0:00:01s\n",
      "epoch 6  | loss: 0.08006 |  0:00:01s\n",
      "epoch 7  | loss: 0.07605 |  0:00:02s\n",
      "epoch 8  | loss: 0.0691  |  0:00:02s\n",
      "epoch 9  | loss: 0.06847 |  0:00:02s\n",
      "epoch 10 | loss: 0.06747 |  0:00:02s\n",
      "epoch 11 | loss: 0.06348 |  0:00:03s\n",
      "epoch 12 | loss: 0.06278 |  0:00:03s\n",
      "epoch 13 | loss: 0.0618  |  0:00:03s\n",
      "epoch 14 | loss: 0.06272 |  0:00:03s\n",
      "epoch 15 | loss: 0.06171 |  0:00:04s\n",
      "epoch 16 | loss: 0.06226 |  0:00:04s\n",
      "epoch 17 | loss: 0.06162 |  0:00:04s\n",
      "epoch 18 | loss: 0.06016 |  0:00:04s\n",
      "epoch 19 | loss: 0.06137 |  0:00:04s\n",
      "epoch 20 | loss: 0.06065 |  0:00:05s\n",
      "epoch 21 | loss: 0.06013 |  0:00:05s\n",
      "epoch 22 | loss: 0.05992 |  0:00:05s\n",
      "epoch 23 | loss: 0.05985 |  0:00:05s\n",
      "epoch 24 | loss: 0.05954 |  0:00:06s\n",
      "epoch 25 | loss: 0.0593  |  0:00:06s\n",
      "epoch 26 | loss: 0.0593  |  0:00:06s\n",
      "epoch 27 | loss: 0.05982 |  0:00:06s\n",
      "epoch 28 | loss: 0.05893 |  0:00:07s\n",
      "epoch 29 | loss: 0.05719 |  0:00:07s\n",
      "epoch 30 | loss: 0.05787 |  0:00:07s\n",
      "epoch 31 | loss: 0.05936 |  0:00:07s\n",
      "epoch 32 | loss: 0.0587  |  0:00:08s\n",
      "epoch 33 | loss: 0.05759 |  0:00:08s\n",
      "epoch 34 | loss: 0.05795 |  0:00:08s\n",
      "epoch 35 | loss: 0.05798 |  0:00:08s\n",
      "epoch 36 | loss: 0.05753 |  0:00:08s\n",
      "epoch 37 | loss: 0.05931 |  0:00:09s\n",
      "epoch 38 | loss: 0.05769 |  0:00:09s\n",
      "epoch 39 | loss: 0.0583  |  0:00:09s\n",
      "epoch 40 | loss: 0.05785 |  0:00:09s\n",
      "epoch 41 | loss: 0.05828 |  0:00:10s\n",
      "epoch 42 | loss: 0.05738 |  0:00:10s\n",
      "epoch 43 | loss: 0.05738 |  0:00:10s\n",
      "epoch 44 | loss: 0.05719 |  0:00:10s\n",
      "epoch 45 | loss: 0.05795 |  0:00:11s\n",
      "epoch 46 | loss: 0.05716 |  0:00:11s\n",
      "epoch 47 | loss: 0.05746 |  0:00:11s\n",
      "epoch 48 | loss: 0.05744 |  0:00:11s\n",
      "epoch 49 | loss: 0.05732 |  0:00:12s\n",
      "epoch 50 | loss: 0.05764 |  0:00:12s\n",
      "epoch 51 | loss: 0.05734 |  0:00:12s\n",
      "epoch 52 | loss: 0.05716 |  0:00:12s\n",
      "epoch 53 | loss: 0.05752 |  0:00:12s\n",
      "epoch 54 | loss: 0.05685 |  0:00:13s\n",
      "epoch 55 | loss: 0.05705 |  0:00:13s\n",
      "epoch 56 | loss: 0.05657 |  0:00:13s\n",
      "epoch 57 | loss: 0.05661 |  0:00:13s\n",
      "epoch 58 | loss: 0.05638 |  0:00:14s\n",
      "epoch 59 | loss: 0.05704 |  0:00:14s\n",
      "epoch 60 | loss: 0.05727 |  0:00:14s\n",
      "epoch 61 | loss: 0.05702 |  0:00:14s\n",
      "epoch 62 | loss: 0.05636 |  0:00:14s\n",
      "epoch 63 | loss: 0.05674 |  0:00:15s\n",
      "epoch 64 | loss: 0.05693 |  0:00:15s\n",
      "epoch 65 | loss: 0.0557  |  0:00:15s\n",
      "epoch 66 | loss: 0.05708 |  0:00:15s\n",
      "epoch 67 | loss: 0.05625 |  0:00:16s\n",
      "epoch 68 | loss: 0.05703 |  0:00:16s\n",
      "epoch 69 | loss: 0.05637 |  0:00:16s\n",
      "epoch 70 | loss: 0.05659 |  0:00:16s\n",
      "epoch 71 | loss: 0.05561 |  0:00:16s\n",
      "epoch 72 | loss: 0.05757 |  0:00:17s\n",
      "epoch 73 | loss: 0.05662 |  0:00:17s\n",
      "epoch 74 | loss: 0.05576 |  0:00:17s\n",
      "epoch 75 | loss: 0.05616 |  0:00:17s\n",
      "epoch 76 | loss: 0.0561  |  0:00:18s\n",
      "epoch 77 | loss: 0.05508 |  0:00:18s\n",
      "epoch 78 | loss: 0.05582 |  0:00:18s\n",
      "epoch 79 | loss: 0.05594 |  0:00:18s\n",
      "epoch 80 | loss: 0.05593 |  0:00:18s\n",
      "epoch 81 | loss: 0.05559 |  0:00:19s\n",
      "epoch 82 | loss: 0.05602 |  0:00:19s\n",
      "epoch 83 | loss: 0.05534 |  0:00:19s\n",
      "epoch 84 | loss: 0.05523 |  0:00:19s\n",
      "epoch 85 | loss: 0.05529 |  0:00:20s\n",
      "epoch 86 | loss: 0.05624 |  0:00:20s\n",
      "epoch 87 | loss: 0.05509 |  0:00:20s\n",
      "epoch 88 | loss: 0.0554  |  0:00:20s\n",
      "epoch 89 | loss: 0.05566 |  0:00:20s\n",
      "epoch 90 | loss: 0.05493 |  0:00:21s\n",
      "epoch 91 | loss: 0.05441 |  0:00:21s\n",
      "epoch 92 | loss: 0.05451 |  0:00:21s\n",
      "epoch 93 | loss: 0.05453 |  0:00:21s\n",
      "epoch 94 | loss: 0.05483 |  0:00:21s\n",
      "epoch 95 | loss: 0.05491 |  0:00:22s\n",
      "epoch 96 | loss: 0.05476 |  0:00:22s\n",
      "epoch 97 | loss: 0.05493 |  0:00:22s\n",
      "epoch 98 | loss: 0.05483 |  0:00:22s\n",
      "epoch 99 | loss: 0.05402 |  0:00:23s\n",
      "epoch 100| loss: 0.05446 |  0:00:23s\n",
      "epoch 101| loss: 0.0548  |  0:00:23s\n",
      "epoch 102| loss: 0.05491 |  0:00:23s\n",
      "epoch 103| loss: 0.05442 |  0:00:23s\n",
      "epoch 104| loss: 0.05491 |  0:00:24s\n",
      "epoch 105| loss: 0.05408 |  0:00:24s\n",
      "epoch 106| loss: 0.05373 |  0:00:24s\n",
      "epoch 107| loss: 0.05384 |  0:00:24s\n",
      "epoch 108| loss: 0.05411 |  0:00:24s\n",
      "epoch 109| loss: 0.05361 |  0:00:25s\n",
      "epoch 110| loss: 0.05274 |  0:00:25s\n",
      "epoch 111| loss: 0.05268 |  0:00:25s\n",
      "epoch 112| loss: 0.05338 |  0:00:25s\n",
      "epoch 113| loss: 0.05359 |  0:00:26s\n",
      "epoch 114| loss: 0.05304 |  0:00:26s\n",
      "epoch 115| loss: 0.05261 |  0:00:26s\n",
      "epoch 116| loss: 0.05348 |  0:00:26s\n",
      "epoch 117| loss: 0.05323 |  0:00:26s\n",
      "epoch 118| loss: 0.053   |  0:00:27s\n",
      "epoch 119| loss: 0.05326 |  0:00:27s\n",
      "epoch 120| loss: 0.05269 |  0:00:27s\n",
      "epoch 121| loss: 0.05338 |  0:00:27s\n",
      "epoch 122| loss: 0.05346 |  0:00:28s\n",
      "epoch 123| loss: 0.05229 |  0:00:28s\n",
      "epoch 124| loss: 0.05292 |  0:00:28s\n",
      "epoch 125| loss: 0.0528  |  0:00:28s\n",
      "epoch 126| loss: 0.053   |  0:00:28s\n",
      "epoch 127| loss: 0.05264 |  0:00:29s\n",
      "epoch 128| loss: 0.05248 |  0:00:29s\n",
      "epoch 129| loss: 0.05287 |  0:00:29s\n",
      "epoch 130| loss: 0.05279 |  0:00:29s\n",
      "epoch 131| loss: 0.05292 |  0:00:30s\n",
      "epoch 132| loss: 0.05251 |  0:00:30s\n",
      "epoch 133| loss: 0.05281 |  0:00:30s\n",
      "epoch 134| loss: 0.05153 |  0:00:30s\n",
      "epoch 135| loss: 0.05297 |  0:00:30s\n",
      "epoch 136| loss: 0.05245 |  0:00:31s\n",
      "epoch 137| loss: 0.05196 |  0:00:31s\n",
      "epoch 138| loss: 0.05217 |  0:00:31s\n",
      "epoch 139| loss: 0.05289 |  0:00:31s\n",
      "epoch 140| loss: 0.05242 |  0:00:31s\n",
      "epoch 141| loss: 0.0515  |  0:00:32s\n",
      "epoch 142| loss: 0.05292 |  0:00:32s\n",
      "epoch 143| loss: 0.05259 |  0:00:32s\n",
      "epoch 144| loss: 0.05123 |  0:00:32s\n",
      "epoch 145| loss: 0.05168 |  0:00:33s\n",
      "epoch 146| loss: 0.05133 |  0:00:33s\n",
      "epoch 147| loss: 0.05177 |  0:00:33s\n",
      "epoch 148| loss: 0.05106 |  0:00:33s\n",
      "epoch 149| loss: 0.05253 |  0:00:33s\n",
      "epoch 150| loss: 0.05145 |  0:00:34s\n",
      "epoch 151| loss: 0.05214 |  0:00:34s\n",
      "epoch 152| loss: 0.05237 |  0:00:34s\n",
      "epoch 153| loss: 0.05174 |  0:00:34s\n",
      "epoch 154| loss: 0.05122 |  0:00:34s\n",
      "epoch 155| loss: 0.05121 |  0:00:35s\n",
      "epoch 156| loss: 0.04976 |  0:00:35s\n",
      "epoch 157| loss: 0.05081 |  0:00:35s\n",
      "epoch 158| loss: 0.05049 |  0:00:35s\n",
      "epoch 159| loss: 0.05151 |  0:00:36s\n",
      "epoch 160| loss: 0.05004 |  0:00:36s\n",
      "epoch 161| loss: 0.05102 |  0:00:36s\n",
      "epoch 162| loss: 0.05028 |  0:00:36s\n",
      "epoch 163| loss: 0.05074 |  0:00:36s\n",
      "epoch 164| loss: 0.05122 |  0:00:37s\n",
      "epoch 165| loss: 0.05127 |  0:00:37s\n",
      "epoch 166| loss: 0.05105 |  0:00:37s\n",
      "epoch 167| loss: 0.05045 |  0:00:37s\n",
      "epoch 168| loss: 0.05085 |  0:00:38s\n",
      "epoch 169| loss: 0.05056 |  0:00:38s\n",
      "epoch 170| loss: 0.05057 |  0:00:38s\n",
      "epoch 171| loss: 0.05084 |  0:00:38s\n",
      "epoch 172| loss: 0.04976 |  0:00:38s\n",
      "epoch 173| loss: 0.05044 |  0:00:39s\n",
      "epoch 174| loss: 0.05176 |  0:00:39s\n",
      "epoch 175| loss: 0.05006 |  0:00:39s\n",
      "epoch 176| loss: 0.04957 |  0:00:39s\n",
      "epoch 177| loss: 0.04942 |  0:00:40s\n",
      "epoch 178| loss: 0.04875 |  0:00:40s\n",
      "epoch 179| loss: 0.05052 |  0:00:40s\n",
      "epoch 180| loss: 0.0497  |  0:00:40s\n",
      "epoch 181| loss: 0.0497  |  0:00:40s\n",
      "epoch 182| loss: 0.04998 |  0:00:41s\n",
      "epoch 183| loss: 0.05044 |  0:00:41s\n",
      "epoch 184| loss: 0.05004 |  0:00:41s\n",
      "epoch 185| loss: 0.04905 |  0:00:41s\n",
      "epoch 186| loss: 0.04949 |  0:00:42s\n",
      "epoch 187| loss: 0.04984 |  0:00:42s\n",
      "epoch 188| loss: 0.0485  |  0:00:42s\n",
      "epoch 189| loss: 0.04948 |  0:00:42s\n",
      "epoch 190| loss: 0.04989 |  0:00:43s\n",
      "epoch 191| loss: 0.04952 |  0:00:43s\n",
      "epoch 192| loss: 0.05027 |  0:00:43s\n",
      "epoch 193| loss: 0.04949 |  0:00:43s\n",
      "epoch 194| loss: 0.05016 |  0:00:43s\n",
      "epoch 195| loss: 0.04852 |  0:00:44s\n",
      "epoch 196| loss: 0.04952 |  0:00:44s\n",
      "epoch 197| loss: 0.04949 |  0:00:44s\n",
      "epoch 198| loss: 0.04942 |  0:00:44s\n",
      "epoch 199| loss: 0.04917 |  0:00:45s\n",
      "epoch 200| loss: 0.04956 |  0:00:45s\n",
      "epoch 201| loss: 0.0488  |  0:00:45s\n",
      "epoch 202| loss: 0.04866 |  0:00:45s\n",
      "epoch 203| loss: 0.04819 |  0:00:46s\n",
      "epoch 204| loss: 0.04862 |  0:00:46s\n",
      "epoch 205| loss: 0.04848 |  0:00:46s\n",
      "epoch 206| loss: 0.04893 |  0:00:46s\n",
      "epoch 207| loss: 0.04931 |  0:00:46s\n",
      "epoch 208| loss: 0.04811 |  0:00:47s\n",
      "epoch 209| loss: 0.04807 |  0:00:47s\n",
      "epoch 210| loss: 0.04828 |  0:00:47s\n",
      "epoch 211| loss: 0.04854 |  0:00:48s\n",
      "epoch 212| loss: 0.04805 |  0:00:48s\n",
      "epoch 213| loss: 0.04742 |  0:00:48s\n",
      "epoch 214| loss: 0.04758 |  0:00:48s\n",
      "epoch 215| loss: 0.04878 |  0:00:48s\n",
      "epoch 216| loss: 0.04752 |  0:00:49s\n",
      "epoch 217| loss: 0.04868 |  0:00:49s\n",
      "epoch 218| loss: 0.04875 |  0:00:49s\n",
      "epoch 219| loss: 0.04821 |  0:00:49s\n",
      "epoch 220| loss: 0.04697 |  0:00:49s\n",
      "epoch 221| loss: 0.04644 |  0:00:50s\n",
      "epoch 222| loss: 0.04666 |  0:00:50s\n",
      "epoch 223| loss: 0.04633 |  0:00:50s\n",
      "epoch 224| loss: 0.04626 |  0:00:50s\n",
      "epoch 225| loss: 0.04698 |  0:00:50s\n",
      "epoch 226| loss: 0.04723 |  0:00:51s\n",
      "epoch 227| loss: 0.04627 |  0:00:51s\n",
      "epoch 228| loss: 0.04676 |  0:00:51s\n",
      "epoch 229| loss: 0.04754 |  0:00:51s\n",
      "epoch 230| loss: 0.04687 |  0:00:52s\n",
      "epoch 231| loss: 0.04688 |  0:00:52s\n",
      "epoch 232| loss: 0.04695 |  0:00:52s\n",
      "epoch 233| loss: 0.04665 |  0:00:52s\n",
      "epoch 234| loss: 0.04629 |  0:00:52s\n",
      "epoch 235| loss: 0.04499 |  0:00:53s\n",
      "epoch 236| loss: 0.04678 |  0:00:53s\n",
      "epoch 237| loss: 0.04685 |  0:00:53s\n",
      "epoch 238| loss: 0.04636 |  0:00:53s\n",
      "epoch 239| loss: 0.04679 |  0:00:53s\n",
      "epoch 240| loss: 0.04638 |  0:00:54s\n",
      "epoch 241| loss: 0.04592 |  0:00:54s\n",
      "epoch 242| loss: 0.0453  |  0:00:54s\n",
      "epoch 243| loss: 0.04622 |  0:00:54s\n",
      "epoch 244| loss: 0.04631 |  0:00:54s\n",
      "epoch 245| loss: 0.04553 |  0:00:55s\n",
      "epoch 246| loss: 0.04698 |  0:00:55s\n",
      "epoch 247| loss: 0.04496 |  0:00:55s\n",
      "epoch 248| loss: 0.0458  |  0:00:55s\n",
      "epoch 249| loss: 0.04564 |  0:00:56s\n",
      "epoch 250| loss: 0.04614 |  0:00:56s\n",
      "epoch 251| loss: 0.04566 |  0:00:56s\n",
      "epoch 252| loss: 0.04571 |  0:00:56s\n",
      "epoch 253| loss: 0.04541 |  0:00:56s\n",
      "epoch 254| loss: 0.04645 |  0:00:57s\n",
      "epoch 255| loss: 0.04597 |  0:00:57s\n",
      "epoch 256| loss: 0.046   |  0:00:57s\n",
      "epoch 257| loss: 0.04586 |  0:00:57s\n",
      "epoch 258| loss: 0.04617 |  0:00:58s\n",
      "epoch 259| loss: 0.04623 |  0:00:58s\n",
      "epoch 260| loss: 0.04542 |  0:00:58s\n",
      "epoch 261| loss: 0.0459  |  0:00:58s\n",
      "epoch 262| loss: 0.04613 |  0:00:58s\n",
      "epoch 263| loss: 0.04502 |  0:00:59s\n",
      "epoch 264| loss: 0.04384 |  0:00:59s\n",
      "epoch 265| loss: 0.04523 |  0:00:59s\n",
      "epoch 266| loss: 0.04409 |  0:00:59s\n",
      "epoch 267| loss: 0.04446 |  0:00:59s\n",
      "epoch 268| loss: 0.04458 |  0:01:00s\n",
      "epoch 269| loss: 0.04498 |  0:01:00s\n",
      "epoch 270| loss: 0.04536 |  0:01:00s\n",
      "epoch 271| loss: 0.04593 |  0:01:00s\n",
      "epoch 272| loss: 0.04498 |  0:01:01s\n",
      "epoch 273| loss: 0.0433  |  0:01:01s\n",
      "epoch 274| loss: 0.04341 |  0:01:01s\n",
      "epoch 275| loss: 0.04408 |  0:01:01s\n",
      "epoch 276| loss: 0.04462 |  0:01:01s\n",
      "epoch 277| loss: 0.04406 |  0:01:02s\n",
      "epoch 278| loss: 0.04491 |  0:01:02s\n",
      "epoch 279| loss: 0.04512 |  0:01:02s\n",
      "epoch 280| loss: 0.04379 |  0:01:02s\n",
      "epoch 281| loss: 0.04432 |  0:01:03s\n",
      "epoch 282| loss: 0.0437  |  0:01:03s\n",
      "epoch 283| loss: 0.04434 |  0:01:03s\n",
      "epoch 284| loss: 0.04342 |  0:01:03s\n",
      "epoch 285| loss: 0.045   |  0:01:03s\n",
      "epoch 286| loss: 0.04355 |  0:01:04s\n",
      "epoch 287| loss: 0.04353 |  0:01:04s\n",
      "epoch 288| loss: 0.04423 |  0:01:04s\n",
      "epoch 289| loss: 0.04525 |  0:01:04s\n",
      "epoch 290| loss: 0.04436 |  0:01:04s\n",
      "epoch 291| loss: 0.04428 |  0:01:05s\n",
      "epoch 292| loss: 0.04353 |  0:01:05s\n",
      "epoch 293| loss: 0.04318 |  0:01:05s\n",
      "epoch 294| loss: 0.04364 |  0:01:05s\n",
      "epoch 295| loss: 0.04351 |  0:01:06s\n",
      "epoch 296| loss: 0.04204 |  0:01:06s\n",
      "epoch 297| loss: 0.04314 |  0:01:06s\n",
      "epoch 298| loss: 0.04336 |  0:01:06s\n",
      "epoch 299| loss: 0.04214 |  0:01:06s\n"
     ]
    }
   ],
   "source": [
    "model2.fit(\n",
    "    X_train=X_train_sub.values,\n",
    "    y_train=y_train_sub.values.reshape(-1, 1),\n",
    "    max_epochs=300,\n",
    "    patience=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(X_val.values)\n",
    "y_val_pred = y_val_pred.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TabNet 검증 MAE: 0.1625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_val.values, y_val_pred)\n",
    "print(f\"✅ TabNet 검증 MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
